Ensuring environment is set up...
'circuit-tracer' library found.
Hugging Face token found.
Training model gemma-2-2b on 8 GPUs.
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
WARNING:root:float16 models may not work on CPU. Consider using a GPU or bfloat16.
WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
WARNING:root:float16 models may not work on CPU. Consider using a GPU or bfloat16.
WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
WARNING:root:float16 models may not work on CPU. Consider using a GPU or bfloat16.
WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
WARNING:root:float16 models may not work on CPU. Consider using a GPU or bfloat16.
WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
WARNING:root:float16 models may not work on CPU. Consider using a GPU or bfloat16.
WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
WARNING:root:float16 models may not work on CPU. Consider using a GPU or bfloat16.
WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
WARNING:root:float16 models may not work on CPU. Consider using a GPU or bfloat16.
WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
WARNING:root:float16 models may not work on CPU. Consider using a GPU or bfloat16.
WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
RANK 4: ABOUT TO CALL THE NEW load_session with correct arguments.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
RANK 1: ABOUT TO CALL THE NEW load_session with correct arguments.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
RANK 5: ABOUT TO CALL THE NEW load_session with correct arguments.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
RANK 3: ABOUT TO CALL THE NEW load_session with correct arguments.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
About to start training with lr 0.0004 and l1 0.00014
Checkpoint path: /home/rickpereira/output/gemma-2-2b/wapcg2we
RANK 0: ABOUT TO CALL THE NEW load_session with correct arguments.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
RANK 6: ABOUT TO CALL THE NEW load_session with correct arguments.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
RANK 7: ABOUT TO CALL THE NEW load_session with correct arguments.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
Run name: 73728-L1-0.00014-LR-0.0004-Tokens-6.000e+07
n_tokens_per_buffer (millions): 0.008192
Lower bound: n_contexts_per_buffer (millions): 0.000256
Total training steps: 29296
Total wandb updates: 2929
n_tokens_per_feature_sampling_window (millions): 65.536
n_tokens_per_dead_feature_window (millions): 327.68
Using Ghost Grads.
We will reset the sparsity calculation 29 times.
Number of tokens when resampling: 8224
Number tokens in sparsity calculation window: 2.05e+06
RANK 2: ABOUT TO CALL THE NEW load_session with correct arguments.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|      | 1/3 [00:01<00:02,  1.19s/it]Loading checkpoint shards:  33%|      | 1/3 [00:02<00:04,  2.16s/it]Loading checkpoint shards:  33%|      | 1/3 [00:02<00:04,  2.36s/it]Loading checkpoint shards:  33%|      | 1/3 [00:02<00:05,  2.98s/it]Loading checkpoint shards:  33%|      | 1/3 [00:03<00:06,  3.10s/it]Loading checkpoint shards:  33%|      | 1/3 [00:02<00:05,  3.00s/it]Loading checkpoint shards:  33%|      | 1/3 [00:03<00:06,  3.02s/it]Loading checkpoint shards:  33%|      | 1/3 [00:02<00:05,  3.00s/it]Loading checkpoint shards:  67%|   | 2/3 [00:05<00:02,  2.61s/it]Loading checkpoint shards: 100%|| 3/3 [00:05<00:00,  1.62s/it]Loading checkpoint shards: 100%|| 3/3 [00:05<00:00,  1.84s/it]
Loading checkpoint shards:  67%|   | 2/3 [00:06<00:03,  3.48s/it]Loading checkpoint shards: 100%|| 3/3 [00:06<00:00,  2.04s/it]Loading checkpoint shards: 100%|| 3/3 [00:06<00:00,  2.20s/it]
Loading checkpoint shards:  67%|   | 2/3 [00:07<00:03,  3.79s/it]Loading checkpoint shards: 100%|| 3/3 [00:07<00:00,  2.21s/it]Loading checkpoint shards: 100%|| 3/3 [00:07<00:00,  2.50s/it]
Loading checkpoint shards:  67%|   | 2/3 [00:07<00:03,  3.87s/it]Loading checkpoint shards: 100%|| 3/3 [00:07<00:00,  2.50s/it]
Loading checkpoint shards:  67%|   | 2/3 [00:07<00:03,  3.93s/it]Loading checkpoint shards:  67%|   | 2/3 [00:07<00:03,  3.89s/it]Loading checkpoint shards:  67%|   | 2/3 [00:07<00:03,  3.87s/it]Loading checkpoint shards:  67%|   | 2/3 [00:07<00:03,  3.88s/it]Loading checkpoint shards: 100%|| 3/3 [00:07<00:00,  2.55s/it]
Loading checkpoint shards: 100%|| 3/3 [00:07<00:00,  2.53s/it]
Loading checkpoint shards: 100%|| 3/3 [00:07<00:00,  2.17s/it]Loading checkpoint shards: 100%|| 3/3 [00:07<00:00,  2.54s/it]
Loading checkpoint shards: 100%|| 3/3 [00:07<00:00,  2.15s/it]Loading checkpoint shards: 100%|| 3/3 [00:07<00:00,  2.53s/it]
WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.
WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.
WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping
WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping
WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.
WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.
WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.
WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.
WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.
WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.
WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping
WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping
WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping
WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping
WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping
WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping
Loaded pretrained model gemma-2-2b into HookedTransformer
Moving model to device:  cuda
Dataset tokenization status: False
Rank 0 filling buffer:   0%|          | 0/16 [00:00<?, ?it/s]Rank 0 filling buffer:   6%|         | 1/16 [00:00<00:07,  2.08it/s]Rank 0 filling buffer:  31%|      | 5/16 [00:00<00:01, 10.51it/s]Rank 0 filling buffer:  62%|   | 10/16 [00:00<00:00, 19.39it/s]Rank 0 filling buffer:  94%|| 15/16 [00:00<00:00, 26.76it/s]Rank 0 filling buffer: 100%|| 16/16 [00:00<00:00, 19.26it/s]

--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0


--- [Rank 0] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:0, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:0, shape=torch.Size([32])
  Other: (device):   device=cuda:0

Rank 0 filling buffer:   0%|          | 0/16 [00:00<?, ?it/s]Rank 0 filling buffer:  25%|       | 4/16 [00:00<00:00, 37.53it/s]Rank 0 filling buffer:  56%|    | 9/16 [00:00<00:00, 41.70it/s]Rank 0 filling buffer:  88%| | 14/16 [00:00<00:00, 42.11it/s]Rank 0 filling buffer: 100%|| 16/16 [00:00<00:00, 40.91it/s]
Reinitialized b_dec with mean of activations across 8 GPUs.
Reinitialized b_dec_out with mean of activations across 8 GPUs.
Training SAE:   0%|          | 0/60000000 [00:00<?, ?it/s]0| MSE Loss 0.037 | L1 0.943:   0%|          | 0/60000000 [00:00<?, ?it/s]0| MSE Loss 0.037 | L1 0.943:   0%|          | 16384/60000000 [00:00<27:29, 36372.48it/s]1| MSE Loss nan | L1 nan:   0%|          | 16384/60000000 [00:01<27:29, 36372.48it/s]    1| MSE Loss nan | L1 nan:   0%|          | 32768/60000000 [00:01<46:27, 21515.38it/s]Loaded pretrained model gemma-2-2b into HookedTransformer
Moving model to device:  cuda

--- [Rank 2] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:2, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:2, shape=torch.Size([32])
  Other: (device):   device=cuda:2


--- [Rank 2] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:2, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:2, shape=torch.Size([32])
  Other: (device):   device=cuda:2


--- [Rank 2] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:2, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:2, shape=torch.Size([32])
  Other: (device):   device=cuda:2


--- [Rank 2] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:2, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:2, shape=torch.Size([32])
  Other: (device):   device=cuda:2


--- [Rank 2] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:2, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:2, shape=torch.Size([32])
  Other: (device):   device=cuda:2


--- [Rank 2] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:2, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:2, shape=torch.Size([32])
  Other: (device):   device=cuda:2


--- [Rank 2] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:2, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:2, shape=torch.Size([32])
  Other: (device):   device=cuda:2


--- [Rank 2] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:2, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:2, shape=torch.Size([32])
  Other: (device):   device=cuda:2


--- [Rank 2] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:2, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:2, shape=torch.Size([32])
  Other: (device):   device=cuda:2


--- [Rank 2] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:2, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:2, shape=torch.Size([32])
  Other: (device):   device=cuda:2


--- [Rank 2] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:2, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:2, shape=torch.Size([32])
  Other: (device):   device=cuda:2


--- [Rank 2] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:2, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:2, shape=torch.Size([32])
  Other: (device):   device=cuda:2


--- [Rank 2] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:2, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:2, shape=torch.Size([32])
  Other: (device):   device=cuda:2


--- [Rank 2] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:2, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:2, shape=torch.Size([32])
  Other: (device):   device=cuda:2


--- [Rank 2] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:2, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:2, shape=torch.Size([32])
  Other: (device):   device=cuda:2


--- [Rank 2] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:2, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:2, shape=torch.Size([32])
  Other: (device):   device=cuda:2

Loaded pretrained model gemma-2-2b into HookedTransformer
Moving model to device:  cuda

--- [Rank 7] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:7, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:7, shape=torch.Size([32])
  Other: (device):   device=cuda:7


--- [Rank 7] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:7, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:7, shape=torch.Size([32])
  Other: (device):   device=cuda:7


--- [Rank 7] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:7, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:7, shape=torch.Size([32])
  Other: (device):   device=cuda:7


--- [Rank 7] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:7, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:7, shape=torch.Size([32])
  Other: (device):   device=cuda:7


--- [Rank 7] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:7, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:7, shape=torch.Size([32])
  Other: (device):   device=cuda:7


--- [Rank 7] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:7, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:7, shape=torch.Size([32])
  Other: (device):   device=cuda:7


--- [Rank 7] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:7, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:7, shape=torch.Size([32])
  Other: (device):   device=cuda:7


--- [Rank 7] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:7, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:7, shape=torch.Size([32])
  Other: (device):   device=cuda:7


--- [Rank 7] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:7, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:7, shape=torch.Size([32])
  Other: (device):   device=cuda:7


--- [Rank 7] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:7, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:7, shape=torch.Size([32])
  Other: (device):   device=cuda:7


--- [Rank 7] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:7, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:7, shape=torch.Size([32])
  Other: (device):   device=cuda:7


--- [Rank 7] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:7, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:7, shape=torch.Size([32])
  Other: (device):   device=cuda:7


--- [Rank 7] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:7, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:7, shape=torch.Size([32])
  Other: (device):   device=cuda:7


--- [Rank 7] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:7, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:7, shape=torch.Size([32])
  Other: (device):   device=cuda:7


--- [Rank 7] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:7, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:7, shape=torch.Size([32])
  Other: (device):   device=cuda:7


--- [Rank 7] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:7, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:7, shape=torch.Size([32])
  Other: (device):   device=cuda:7

Loaded pretrained model gemma-2-2b into HookedTransformer
Moving model to device:  cuda

--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1


--- [Rank 1] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:1, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:1, shape=torch.Size([32])
  Other: (device):   device=cuda:1

Loaded pretrained model gemma-2-2b into HookedTransformer
Moving model to device:  cuda

--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4


--- [Rank 4] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:4, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:4, shape=torch.Size([32])
  Other: (device):   device=cuda:4

Loaded pretrained model gemma-2-2b into HookedTransformer
Moving model to device:  cuda

--- [Rank 3] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:3, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:3, shape=torch.Size([32])
  Other: (device):   device=cuda:3


--- [Rank 3] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:3, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:3, shape=torch.Size([32])
  Other: (device):   device=cuda:3


--- [Rank 3] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:3, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:3, shape=torch.Size([32])
  Other: (device):   device=cuda:3


--- [Rank 3] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:3, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:3, shape=torch.Size([32])
  Other: (device):   device=cuda:3


--- [Rank 3] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:3, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:3, shape=torch.Size([32])
  Other: (device):   device=cuda:3


--- [Rank 3] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:3, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:3, shape=torch.Size([32])
  Other: (device):   device=cuda:3


--- [Rank 3] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:3, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:3, shape=torch.Size([32])
  Other: (device):   device=cuda:3


--- [Rank 3] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:3, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:3, shape=torch.Size([32])
  Other: (device):   device=cuda:3


--- [Rank 3] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:3, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:3, shape=torch.Size([32])
  Other: (device):   device=cuda:3


--- [Rank 3] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:3, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:3, shape=torch.Size([32])
  Other: (device):   device=cuda:3


--- [Rank 3] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:3, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:3, shape=torch.Size([32])
  Other: (device):   device=cuda:3


--- [Rank 3] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:3, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:3, shape=torch.Size([32])
  Other: (device):   device=cuda:3


--- [Rank 3] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:3, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:3, shape=torch.Size([32])
  Other: (device):   device=cuda:3


--- [Rank 3] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:3, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:3, shape=torch.Size([32])
  Other: (device):   device=cuda:3


--- [Rank 3] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:3, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:3, shape=torch.Size([32])
  Other: (device):   device=cuda:3


--- [Rank 3] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:3, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:3, shape=torch.Size([32])
  Other: (device):   device=cuda:3

Loaded pretrained model gemma-2-2b into HookedTransformer
Moving model to device:  cuda

--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6


--- [Rank 6] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:6, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:6, shape=torch.Size([32])
  Other: (device):   device=cuda:6

Loaded pretrained model gemma-2-2b into HookedTransformer
Moving model to device:  cuda

--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([0, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([1, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([2, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([3, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([4, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([5, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([6, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5


--- [Rank 5] Debugging torch.cat --- 
  Culprit 1 (batch_tokens): device=cuda:5, shape=torch.Size([7, 32])
  Culprit 2 (full_batch):   device=cuda:5, shape=torch.Size([32])
  Other: (device):   device=cuda:5

1| MSE Loss nan | L1 nan:   0%|          | 32768/60000000 [00:02<1:07:30, 14805.75it/s]
terminate called after throwing an instance of 'c10::Error'
  what():  CUDA error: uncorrectable ECC error encountered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at /pytorch/c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7f44b89785e8 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xe0 (0x7f44b890d4a2 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x3c2 (0x7f44b8ddc422 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)
frame #3: <unknown function> + 0x1e79f (0x7f44b8da479f in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x20060 (0x7f44b8da6060 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)
frame #5: <unknown function> + 0x2028c (0x7f44b8da628c in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libc10_cuda.so)
frame #6: <unknown function> + 0x4582d2 (0x7f44b02582d2 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #7: c10::TensorImpl::~TensorImpl() + 0x9 (0x7f44b8952f39 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #8: c10d::Reducer::~Reducer() + 0x329 (0x7f44a15daed9 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)
frame #9: <unknown function> + 0xc5b032 (0x7f44b0a5b032 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x37d94a (0x7f44b017d94a in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #11: <unknown function> + 0xc655b1 (0x7f44b0a655b1 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #12: <unknown function> + 0x3885b0 (0x7f44b01885b0 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #13: <unknown function> + 0x388bf1 (0x7f44b0188bf1 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_python.so)
frame #14: /home/rickpereira/circuit-tracer-workspace/bin/python() [0x515e16]
frame #15: /home/rickpereira/circuit-tracer-workspace/bin/python() [0x58c7d5]
frame #16: /home/rickpereira/circuit-tracer-workspace/bin/python() [0x567963]
frame #17: /home/rickpereira/circuit-tracer-workspace/bin/python() [0x5657de]
frame #18: /home/rickpereira/circuit-tracer-workspace/bin/python() [0x56579d]
frame #19: /home/rickpereira/circuit-tracer-workspace/bin/python() [0x578a2d]
frame #20: /home/rickpereira/circuit-tracer-workspace/bin/python() [0x58c44e]
frame #21: /home/rickpereira/circuit-tracer-workspace/bin/python() [0x50d29c]
frame #22: /home/rickpereira/circuit-tracer-workspace/bin/python() [0x6690f1]
frame #23: _PyEval_EvalFrameDefault + 0x7deb (0x532edb in /home/rickpereira/circuit-tracer-workspace/bin/python)
frame #24: PyEval_EvalCode + 0xbb (0x5236bb in /home/rickpereira/circuit-tracer-workspace/bin/python)
frame #25: /home/rickpereira/circuit-tracer-workspace/bin/python() [0x647d97]
frame #26: /home/rickpereira/circuit-tracer-workspace/bin/python() [0x6456ef]
frame #27: PyRun_StringFlags + 0x5d (0x56f02d in /home/rickpereira/circuit-tracer-workspace/bin/python)
frame #28: PyRun_SimpleStringFlags + 0x36 (0x63ed66 in /home/rickpereira/circuit-tracer-workspace/bin/python)
frame #29: Py_RunMain + 0x454 (0x6502c4 in /home/rickpereira/circuit-tracer-workspace/bin/python)
frame #30: Py_BytesMain + 0x27 (0x627d37 in /home/rickpereira/circuit-tracer-workspace/bin/python)
frame #31: <unknown function> + 0x2724a (0x7f44b969424a in /lib/x86_64-linux-gnu/libc.so.6)
frame #32: __libc_start_main + 0x85 (0x7f44b9694305 in /lib/x86_64-linux-gnu/libc.so.6)
frame #33: _start + 0x21 (0x627bd1 in /home/rickpereira/circuit-tracer-workspace/bin/python)

[rank5]:[W614 21:49:40.964607304 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=6, addr=[localhost]:43946, remote=[localhost]:12355): failed to recv, got 0 bytes
Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7efdd09785e8 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8afe (0x7efdb95a8afe in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5baae40 (0x7efdb95aae40 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5bab74a (0x7efdb95ab74a in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7efdb95a51a9 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7efd765e09a9 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xd44a3 (0x7efd660d44a3 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #7: <unknown function> + 0x891f5 (0x7efdd16961f5 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x10989c (0x7efdd171689c in /lib/x86_64-linux-gnu/libc.so.6)

[rank5]:[W614 21:49:40.968530204 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0(default_pg) Rank 5] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes
[rank1]:[W614 21:49:40.965624400 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=8, addr=[localhost]:43968, remote=[localhost]:12355): failed to recv, got 0 bytes
Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fd51fd785e8 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8afe (0x7fd508da8afe in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5baae40 (0x7fd508daae40 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5bab74a (0x7fd508dab74a in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7fd508da51a9 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7fd4c5de09a9 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xd44a3 (0x7fd4b58d44a3 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #7: <unknown function> + 0x891f5 (0x7fd520d981f5 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x10989c (0x7fd520e1889c in /lib/x86_64-linux-gnu/libc.so.6)

[rank3]:[W614 21:49:40.965523476 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=7, addr=[localhost]:44002, remote=[localhost]:12355): failed to recv, got 0 bytes
Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:678 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x98 (0x7fe9367785e8 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5ba8afe (0x7fe91f3a8afe in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5baae40 (0x7fe91f3aae40 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5bab74a (0x7fe91f3ab74a in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x2a9 (0x7fe91f3a51a9 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::heartbeatMonitor() + 0x379 (0x7fe8dc3e09a9 in /home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xd44a3 (0x7fe8cbed44a3 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #7: <unknown function> + 0x891f5 (0x7fe9374a81f5 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x10989c (0x7fe93752889c in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[W614 21:49:40.969438539 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0(default_pg) Rank 1] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes
[rank3]:[W614 21:49:40.969475192 ProcessGroupNCCL.cpp:1659] [PG ID 0 PG GUID 0(default_pg) Rank 3] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: failed to recv, got 0 bytes
W0614 21:49:40.124000 1499157 torch/multiprocessing/spawn.py:169] Terminating process 1499229 via signal SIGTERM
W0614 21:49:40.127000 1499157 torch/multiprocessing/spawn.py:169] Terminating process 1499230 via signal SIGTERM
W0614 21:49:40.129000 1499157 torch/multiprocessing/spawn.py:169] Terminating process 1499231 via signal SIGTERM
W0614 21:49:40.130000 1499157 torch/multiprocessing/spawn.py:169] Terminating process 1499232 via signal SIGTERM
W0614 21:49:40.130000 1499157 torch/multiprocessing/spawn.py:169] Terminating process 1499233 via signal SIGTERM
W0614 21:49:40.132000 1499157 torch/multiprocessing/spawn.py:169] Terminating process 1499234 via signal SIGTERM
W0614 21:49:40.132000 1499157 torch/multiprocessing/spawn.py:169] Terminating process 1499235 via signal SIGTERM
Traceback (most recent call last):
  File "/home/rickpereira/circuit-tracer/scripts/train_transcoder.py", line 287, in <module>
    main()
  File "/home/rickpereira/circuit-tracer/scripts/train_transcoder.py", line 280, in main
    mp.spawn(train_worker,
  File "/home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/multiprocessing/spawn.py", line 340, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/multiprocessing/spawn.py", line 296, in start_processes
    while not context.join():
              ^^^^^^^^^^^^^^
  File "/home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/multiprocessing/spawn.py", line 215, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/multiprocessing/spawn.py", line 90, in _wrap
    fn(i, *args)
  File "/home/rickpereira/circuit-tracer/scripts/train_transcoder.py", line 215, in train_worker
    sparse_autoencoder = train_sae_on_language_model(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rickpereira/circuit-tracer/sae_training/train_sae_on_language_model.py", line 259, in train_sae_on_language_model
    loss.backward()
  File "/home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/rickpereira/circuit-tracer-workspace/lib/python3.11/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.DistBackendError: NCCL error in: /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:3353, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.26.2
ncclUnhandledCudaError: Call to CUDA function failed.
Last error:
Cuda failure 214 'uncorrectable ECC error encountered'

